{"componentChunkName":"component---src-templates-blog-post-tsx","path":"/blog/making-a-plain-text-corpus-of-wikipedia/","result":{"data":{"site":{"siteMetadata":{"author":"Jeff Willette"}},"authorAvatar":{"childImageSharp":{"fluid":{"tracedSVG":"data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='400'%20viewBox='0%200%20400%20400'%20preserveAspectRatio='none'%3e%3cpath%20d='M154%2018l-12%202c-4%202-38%203-47%201-13-3-20-1-13%202%205%202%2010%207%2010%209l1%207%201%205c-2%200-5-4-8-8l-2-4-1%203%201%205%201%203c-1%200-2-1-2-3-3-7-5%200-5%2015-1%2014%205%2024%2016%2026l6%202-5%2013c-11%2025-12%2031-15%2071v11h-3c-6%201-7%206-6%2017v17c-3%2024%201%2049%2011%2059%203%203%204%205%205%2010%206%2032%2010%2038%2037%2067%2015%2016%2017%2018%2017%2021%200%204-7%2018-10%2020-7%205-14%208-26%2010a271%20271%200%200047-1l-3-1-2-21%205%204%206%204v1c-1%200%207%206%209%206v1c-2%203-1%207%203%208%203%200%204-1%202-2l-1-2%202%201%209%201%208%201c1%202%2018%201%2018%200l-1-2-1-4-1-4-1-1c1-2%200-2-2-1-5%201-3-1%204-4l20-14%207-5%2013-12c8-7%208-7%208-4a587%20587%200%2000-4%2030c1%205-5%2012-14%2019l-3%203h43c23%200%2042%200%2041-1l-15-4-21-7-7-3c-5-2-7-3-11-9l-5-7c-1%200-1-11%201-24%201-6%202-8%205-12%2010-10%2020-30%2023-45%202-7%203-8%209-8%2019-3%2025-8%2031-28%206-16%207-25%207-43%200-19-1-24-8-28-3-2-4-2-8-1s-4%201-6-1c-3-4-7-49-5-58%202-11-2-25-10-35-4-5-5-7-3-11%203-3%202-7-2-12l-4-5c0-2-7-13-12-18s-15-11-18-11l-2-1c0-1-11-3-26-4-20-1-31-3-34-4l-27-4-15%202m-9%208l33%206c6%200%208%201%208%202l-2%202c-3%201-2%201%204%203l8%202c-1-3%203-2%2010%202l7%203-10-7-7-5c-1-2-14-5-27-5-5%200-7%200-8-2l-10-2c-6%200-8%200-6%201m-30%202l23%206c2%201%203%201%203%203-1%202%200%203%2010%204%208%200%209-1%201-3-5-1-5-3-1-3l-8-2a323%20323%200%2001-28-5M80%2047c0%206%202%2015%204%2020%203%205%203%205%202-1l-2-11%201%201c0%202%2018%2012%2023%2012%204%200%201-2-5-5-10-3-15-8-22-18-1-1-1-1-1%202m27%205c-2%201-1%202%202%204%203%201%204%202%203%203-2%204%205%207%208%204h3c3%201%203%201%203-1%200-1-9-7-18-11l-1%201m3%2043l-3%205c-4%206-5%2010-7%2021a170%20170%200%2001-5%2025l-1%202-2%2029-2%2031c-2%204-2%206-1%2012%201%207%201%2010-1%2016v3l2-3c3-2%207-4%207-2l-4%206c-6%206-6%208-3%2028%203%2024%208%2042%2016%2052a860%20860%200%200032%2033c-1-7%201-6%206%205%207%2012%2011%2016%2017%2018%209%203%209%203%2011%200%200-2%202-3%203-4%204-1%205%201%202%204-4%204-3%204%206%204%2016%200%2019-2%2032-20%209-11%2012-10%206%201-3%206-3%206%204%200l11-7c7-5%2028-28%2032-35%206-11%2014-29%2016-39%203-12%205-20%207-22l-1%2014v4l5-2%205-1-2-3c-6-8%203-12%2010-5l4%202%203-5%203-6%202-7c8-12%2010-27%204-39-3-8-4-8-6-5l-3%202%203%206c5%2012%204%2021-3%2027-10%209-11%2010-13%2010-1-1-1%200-1%202s-1%203-3%201v-8c1-8%201-8-1-14-3-7-3-10%201-17%204-8%2021-25%2026-26%204%200%204-1-1-3-4-2-4-2-9%201-7%204-22%2021-24%2028-1%207-4%209-8%205-2-2-2-3-2-15v-15c-6-11-6-10-5-23a567%20567%200%20001-34c0-13-1-16-6-21-2-3-4-4-4-3s-7%201-11-1c-10-4-30-6-63-8-40-3-53-3-55%200-1%201-1%201-2-1-1-3-3%200-3%205%200%203%206%2013%208%2013l1%201c0%203-13-1-17-5-3-4-5-10-3-13%201-3%200-5-3-5l-4-1c-2-1-2-1-4%202m4%2087l-3%202c-6%200-15%2011-10%2013l1%202h2c10-4%2012-4%2024-3%2018%201%2031%201%2032-1%202-2-1-7-7-9-4-2-6-2-14-3l-12-1h-13m107%200l-9%201c-11%200-19%202-20%204l-3%202c-3%200-4%206%200%206l3%202c3%201%2034%202%2043%200%206-1%208-1%2021%204%204%202%205%201%203-2l-1-2c2%200-3-7-9-11l-6-4h-22m-30%2019l1%202-1%201v1l1%202%202%201c1%200%201%201-1%202-6%204-6%209%200%205%203-2%205-1%203%202-2%202-3%207-2%207l3-3c4-4%209-8%2011-8s2%201%202%205v6l-5-1h-5l3%202c5%202%2033%202%2039%200l13-2c12-1%2013-2%203-4-6%200-8-1-10-3-4-3-10-5-18-7l-5-2h4c5%200%2013%203%2019%206%204%202%204%202%206%200%204-2%204-3%200-3-2%200-5%200-7-2-7-4-14-5-26-5-9%200-13-1-14-2-2-1-8-1-9%201h-1l-4-2c-2%200-3%200-2%201m-74%203l-8%204-8%203c-3%200-4%201%200%203%202%201%203%201%206-1%207-5%2013-6%2024-6l10-1c0-2-17-4-24-2m28%204c1%202%201%202-2%201h-23l-10%205c-4%202-8%204-11%204-6%202-6%203%201%203%204%200%207%200%2012%202%205%203%206%203%2021%203%2014%200%2021-1%2019-3l-4-1c-4%200-4%200-4-3%200-5%201-5%206%200%207%206%2010%205%204-2-2-3-2-4%201-1s5%200%202-4c-5-5-13-8-12-4m17%203c-3%202-4%2031-1%2035%201%201%202-1%202-17%201-20%201-19-1-18m-87%205c-3%209%201%2039%206%2043h1l1%202c1%202%201%200%201-7%200-11%200-11-3-16-4-6-6-13-5-21%201-6%200-6-1-1m69%2013c-4%202-14%203-22%202-9-2-14-2-16-1-3%201-2%202%203%202l10%202c10%203%2032-1%2032-5h-7m53%202l5%201%2011%202c8%202%2023%201%2029-1%206-3%205-3-7-2h-19c-12-2-19-2-19%200m-41%2028c-4%206-4%2013%200%2015%202%202%200%202-3%201l-5-2c-2%200-1-4%201-7%202-2%202-4%200-4s-6%205-7%208c0%203%204%208%2010%2011s9%204%209%202l4%202c6%206%2011%206%2016%202l3-2-7%201c-6%200-6%200-10-3l-6-4c-4-1-5-13-2-21%203-6%200-5-3%201m44%202l-1%201c-1-1-2%200-2%201h-1c0-2-4-1-5%201%200%203%200%203%204%202%203-1%203-1%205%201%201%203%200%206-2%206-5%200-21%206-21%208h5l5%201-3%201-1%201h7l1-4%201%201%204%202c4%200%2014-4%2011-5l-4%201h-2l1-1%204-8c0-2%201-1%206%202%2010%206%2024%2011%2030%2010%203%200%202-2%200-2l-3-1-9-3c-10-3-16-7-23-12-6-4-7-5-7-3m-61%205c-4%204-6%206-15%2011-10%206-12%209-12%2013%201%205%202%205%204%200%201-3%203-4%207-7%2011-6%2017-11%2019-18%201-3-1-2-3%201m85%2033c0%201-1%202-12%202a139%20139%200%2000-18%200c2-1%202-1%201-2-2-1-10%200-16%202h-9c-5-2-13-3-11-1%202%203%2015%204%2027%203l6-1-5%201c-7%203-18%202-29%200l-15-3-7-1c-1-1-2-1-5%201-4%203-5%205-1%204%202%200%208%204%2011%207l14%2010c15%209%2038%208%2053-3%2016-10%2020-13%2022-13%204%200%204-2-1-5s-5-3-5-1m-78%208c0%204%205%208%2010%208%202%200%202%200%201%201-2%201-2%201%202%203%207%204%2034%204%2043%200l5-4h-3c-2%201-12%202-13%201-2-1%201-2%205-2l5-2%206-1c4%200%2010-3%2010-5s-26%200-32%203h-17a358%20358%200%2000-22-2m17%2024l3%203c7%202%2030%201%2031-3h-34'%20fill='%23d3d3d3'%20fill-rule='evenodd'/%3e%3c/svg%3e","aspectRatio":1,"src":"/static/2c4c167c05e10b4e6891182058715e8e/61fd6/jeff.png","srcSet":"/static/2c4c167c05e10b4e6891182058715e8e/69585/jeff.png 200w,\n/static/2c4c167c05e10b4e6891182058715e8e/61fd6/jeff.png 256w","sizes":"(max-width: 256px) 100vw, 256px"}}},"mdx":{"frontmatter":{"title":"Making a Plain Text Corpus of Wikipedia","createdAt":"2016-03-24T01:13:55.000Z","updatedAt":"2019-01-21T16:24:27.023Z","categories":["Programming","Projects","Python"],"images":null},"fields":{"githubLink":"https://github.com/jeffwillette/jeffwillette.github.io.src/tree/dev/data/blog/making-a-plain-text-corpus-of-wikipedia/index.mdx"},"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Making a Plain Text Corpus of Wikipedia\",\n  \"published\": true,\n  \"createdAt\": \"2016-03-24T01:13:55.000Z\",\n  \"updatedAt\": \"2019-01-21T16:24:27.023Z\",\n  \"categories\": [\"Programming\", \"Projects\", \"Python\"]\n};\n\nvar makeShortcode = function makeShortcode(name) {\n  return function MDXDefaultShortcode(props) {\n    console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\");\n    return mdx(\"div\", props);\n  };\n};\n\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"I needed to get a plaintext corpus of wikipedia for a website I have been working on (langalang.com) I did some\\npreliminary research online and came across this method.... \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://blog.afterthedeadline.com/2009/12/04/generating-a-plain-text-corpus-from-wikipedia/\"\n  }), \"https://blog.afterthedeadline.com/2009/12/04/generating-a-plain-text-corpus-from-wikipedia/\")), mdx(\"p\", null, \"It seemed like a good approach but something always went wrong during step 4. There were so many processes hanging\\nthat almost nothing was getting done. I may be wrong, but I didn't know how to investigate further since the code\\nthat runs it is quite complex and written with a lot of java and php, two languages I am not so familiar with. On top\\nof this, my machine at home was pretty weak and I had to rent expensive servers from Digital Ocean which were costing\\nme money by the hour.\"), mdx(\"p\", null, \"I thought about trying to do it myself, but I thought there had to be people who did this before and already sifted\\nthrough the Wikimedia xml to eliminate unnecessary text.\"), mdx(\"p\", null, \"I decided to have a look at other methods and I came across one that worked a whole lot better for me.\"), mdx(\"h2\", null, \"Gensim\"), mdx(\"p\", null, \"I found the tool here. \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://radimrehurek.com/gensim/corpora/wikicorpus.html\"\n  }), \"https://radimrehurek.com/gensim/corpora/wikicorpus.html\"), \"\\nThis is an extremely powerful tool that can do so much more than I need to. I could have written my own script to use\\nthe methods contained in this file, but I found another module that does almost exactly what I need so I decided to\\nuse that. \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://github.com/piskvorky/sim-shootout\"\n  }), \"https://github.com/piskvorky/sim-shootout\")), mdx(\"h2\", null, \"Step 1: Installing and downloading everything\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-bash\"\n  }), \"#Making a direcotry to work in and changing into it\\nmkdir wikicorpus\\ncd wikicorpus\\n#Creating and acitvating a python virtualenvironment\\nvirtualenv venv\\nsource venv/bin/activate\\n#Installing the gensim module into the virtual environment\\npip install gensim\\n#Downloading the sim-shootout module (it uses gensim itself to make the corpus)\\nwget https://github.com/piskvorky/sim-shootout/archive/master.zip\\nunzip master.zip\\n\")), mdx(\"h2\", null, \"Step 2: Getting What I Need From the \", mdx(\"inlineCode\", {\n    parentName: \"h2\"\n  }, \"sim-shootout\"), \" Methods\"), mdx(\"p\", null, \"This is where you may start to differ. I was after a plain text corpus of all the articles from wikipedia, with all\\nof the markup and non-essential text.\"), mdx(\"p\", null, \"The \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"prepare_shootout.py\"), \" file does what I wanted and so much more. In fact I commented out a ton of code from the\\nmain loop to only isolate what I needed....\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-python\"\n  }), \"if __name__ == '__main__':\\n    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\\n    logging.root.setLevel(level=logging.INFO)\\n    logger.info(\\\"running %s\\\" % ' '.join(sys.argv))\\n\\n    # check and process input arguments\\n    program = os.path.basename(sys.argv[0])\\n    if len(sys.argv) < 3:\\n        print globals()['__doc__'] % locals()\\n        sys.exit(1)\\n    infile, outdir = sys.argv[1:3]\\n    outfile = lambda fname: os.path.join(outdir, fname)\\n\\n    # extract plain text from the XML dump\\n    preprocessed_file = outfile('title_tokens.txt.gz')\\n    if not os.path.exists(preprocessed_file):\\n        id2title = []\\n        with gensim.utils.smart_open(preprocessed_file, 'wb') as fout:\\n            for docno, (title, tokens) in enumerate(convert_wiki(infile, PROCESSES)):\\n                id2title.append(title)\\n                try:\\n                    line = \\\"%s\\\\t%s\\\" % (title, ' '.join(tokens))\\n                    fout.write(\\\"%s\\\\n\\\" % gensim.utils.to_utf8(line)) # make sure we're storing proper utf8\\n                except:\\n                    logger.info(\\\"invalid line at title %s\\\" % title)\\n\\n        #I commented out all of this code because I wasn't after any of this stuff, above is the plain text file only...\\n        '''gensim.utils.pickle(id2title, outfile('id2title'))\\n\\n    # build/load a mapping between tokens (strings) and tokens ids (integers)\\n    dict_file = outfile('dictionary')\\n    if os.path.exists(dict_file):\\n        corpus = ShootoutCorpus()\\n        corpus.input = gensim.utils.smart_open(preprocessed_file)\\n        corpus.dictionary = gensim.corpora.Dictionary.load(dict_file)\\n    else:\\n        corpus = ShootoutCorpus(gensim.utils.smart_open(preprocessed_file))\\n        corpus.dictionary.filter_extremes(no_below=20, no_above=0.1, keep_n=50000)  # remove too rare/too common words\\n        corpus.dictionary.save(dict_file)\\n        corpus.dictionary.save_as_text(dict_file + '.txt')\\n\\n    # build/load TF-IDF model\\n    tfidf_file = outfile('tfidf.model')\\n    if os.path.exists(tfidf_file):\\n        tfidf = gensim.models.TfidfModel.load(tfidf_file)\\n    else:\\n        tfidf = gensim.models.TfidfModel(corpus)\\n        tfidf.save(tfidf_file)\\n\\n    # build/load LSI model, on top of the TF-IDF model\\n    lsi_file = outfile('lsi.model')\\n    if os.path.exists(lsi_file):\\n        lsi = gensim.models.LsiModel.load(lsi_file)\\n    else:\\n        lsi = gensim.models.LsiModel(tfidf[corpus], id2word=corpus.dictionary, num_topics=NUM_TOPICS, chunksize=10000)\\n        lsi.save(lsi_file)\\n\\n    # convert all articles to latent semantic space, store the result as a MatrixMarket file\\n    # normalize all vectors to unit length, to simulate cossim in libraries that only support euclidean distance\\n    vectors_file = os.path.join(outdir, 'lsi_vectors.mm')\\n    gensim.corpora.MmCorpus.serialize(vectors_file, (gensim.matutils.unitvec(vec) for vec in lsi[tfidf[corpus]]))\\n\\n    logger.info(\\\"finished running %s\\\" % program)'''\\n\")), mdx(\"p\", null, \"The active code above will create a \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"title_tokens.txt.gz\"), \" file which will consist of the article title, followed by\\nthe complete article text, all on one line.\"), mdx(\"h2\", null, \"Step 3: Changing the \", mdx(\"inlineCode\", {\n    parentName: \"h2\"\n  }, \"Gensim\"), \" module to fit my needs\"), mdx(\"p\", null, \"The Gensim module takes some liberties in filtering out words that are less than two characters in length. This is no\\ngood for me becuase I want a complete English corpus, not filtering out the one letter words 'A' and 'I.' to do this\\nI had to go int \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"gemsim.utils.py\"), \" and edit the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"simple_preprocess\"), \" method.\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-python\"\n  }), \"#I changed the min_len to 1 to include the words 'I' and 'A'\\ndef simple_preprocess(doc, deacc=False, min_len=1, max_len=15):\\n    \\\"\\\"\\\"\\n    Convert a document into a list of tokens.\\n\\n    This lowercases, tokenizes, de-accents (optional). -- the output are final\\n    tokens = unicode strings, that won't be processed any further.\\n\\n    \\\"\\\"\\\"\\n    tokens = [\\n        token for token in tokenize(doc, lower=True, deacc=deacc, errors='ignore')\\n        if min_len <= len(token) <= max_len and not token.startswith('_')\\n    ]\\n    return tokens\\n\")), mdx(\"p\", null, \"This has another side effect of adding random non-english words to the output of the article text. Things like 'i.e.'\\nget split into 'i e' when punctuation is removed and then they are counted as words in the article. In the next\\nmethod which goes over every article text, counting the occurrences of each word, I eliminate these unwanted one\\nletter words. (There are only two one-letter words in English, I could have done that in this step which would have\\nbeen more efficient but it was easy enough and quick to do it separately.)\"), mdx(\"p\", null, \"The Gensim module also does not count wikipedia articles with less than 50 words, I decided to keep that constraint\\nbut it could also easily be modified.\"), mdx(\"h2\", null, \"Step 4: Going Over The Full Articles and Counting Word Occurrences\"), mdx(\"p\", null, \"I wrote this script to iterate over each word in the plaintext output file and count each occurrence, add them\\ntogether and give a final list of each word and its number of occurrences. One per line\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-python\"\n  }), \"import sys\\nimport operator\\n\\n#Open the input file which is a text file with the article title and text all on one line\\ninfile = open(sys.argv[1])\\n\\ntokens = {}\\nfor line in infile:\\n  line = line.split()\\n  for word in line:\\n    if word in tokens:\\n      tokens[word] += 1\\n    else:\\n      tokens[word] = 1\\ninfile.close()\\n\\ntokens = sorted(tokens.items(), key=operator.itemgetter(1), reverse=True)\\n\\n#argv[2] should be a plain string without .txt\\nout_title = sys.argv[2] + '.txt'\\noutfile = open(out_title, 'w+')\\nfor tup in tokens:\\n  #This is where I skip words that are not 'I' and 'A'\\n  if len(tup[0]) == 1 and (tup[0] != 'i' and tup[0] != 'a'):\\n    pass\\n  else:\\n    outfile.write('%s\\\\t%s\\\\n' % (tup[0], tup[1]))\\noutfile.close()\\n\")), mdx(\"h2\", null, \"Step 5: Preparing the Corpus\"), mdx(\"p\", null, \"At this point I am ready to prepare the corpus, I need to run \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"prepare-shootout.py\"), \" which will process the database\\ndump from wikipedia into a compressed plain-text file. In order to get the database dump you should go to\\n\", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://dumps.wikimedia.org/backup-index.html\"\n  }), \"https://dumps.wikimedia.org/backup-index.html\"), \" and select the wiki\\nyou would like to download. From there you need the file that is labeled 'somewiki-somedate-pages-articles.xml.bz2'...\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-bash\"\n  }), \"#Downloading the wiki dump with wget\\nwget https://dumps.wikimedia.org/enwiki/20160305/enwiki-20160305-pages-articles.xml.bz2\\n\")), mdx(\"p\", null, \"This is the name of the file that I downloaded, wikimedia makes database dumps every so often so the current file\\nwill change all the time. That is a ~12GB download. After it finishes, you are ready to process it...\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-bash\"\n  }), \"#Processing the database dump with prepare-shootout.py\\nmkdir output\\npython sim-shootout-master/prepare-shootout.py enwiki-20160305-pages-articles.xml.bz2 output/\\n\")), mdx(\"p\", null, \"This will take a long time to run depending on how powerful your machine is. I am using a t2.medium Amazon EC2\\ninstance, and it has been running for about 12 hours so far...\"), mdx(\"p\", null, \"I ended up stopping that instance and starting up the smallest of the compute optimized instance which went much\\nfaster. (Around 3-4 hours I believe.)\"), mdx(\"p\", null, \"After running this you should have a file names title_tokens.txt.gz. If you unzip this file it will have the article\\ntitle and the article text with all the unnecessary markup and headings removed. YOu can then use this file to\\nprocess it however you want.\"));\n}\n;\nMDXContent.isMDXComponent = true;","timeToRead":3,"tableOfContents":{"items":[{"url":"#gensim","title":"Gensim"},{"url":"#step-1-installing-and-downloading-everything","title":"Step 1: Installing and downloading everything"},{"url":"#step-2-getting-what-i-need-from-the-sim-shootout-methods","title":"Step 2: Getting What I Need From the sim-shootout Methods"},{"url":"#step-3-changing-the-gensim-module-to-fit-my-needs","title":"Step 3: Changing the Gensim module to fit my needs"},{"url":"#step-4-going-over-the-full-articles-and-counting-word-occurrences","title":"Step 4: Going Over The Full Articles and Counting Word Occurrences"},{"url":"#step-5-preparing-the-corpus","title":"Step 5: Preparing the Corpus"}]},"excerpt":"I needed to get a plaintext corpus of wikipedia for a website I have been working on (langalang.com) I did some\npreliminary research onlineâ€¦"}},"pageContext":{"id":"2e1011c9-8a57-5c5a-b678-94449618c979"}}}